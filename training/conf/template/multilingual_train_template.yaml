# train_{{ perturbation }}_{{ train_set }}_{{ paren_model }}{{ no_pos_encodings_underscore }}_seed{{ seed }}.yaml
#   Based on mistral-small.yaml.
---
# Inherit Dataset, Tokenization, Model, and Training Details
inherit:
    - datasets/dataset_{{ perturbation }}_{{ train_set }}_seed{{ seed }}.yaml
    - models/gpt2{{ no_pos_encodings }}-small-{{ perturbation }}-{{ paren_model }}.yaml
    - trainers/gpt2-small.yaml

# Run ID -- make sure to override!
run_id: {{ perturbation }}_{{ train_set }}_{{ paren_model }}{{ no_pos_encodings_underscore }}_seed{{ seed }}

# Weights & Biases
wandb: xiulin-yang-compling
group: xiulin-yang

# Artifacts & Caching
artifacts:
    cache_dir: {{ ckpt_path }}/{{ perturbation }}_{{ train_set }}_{{ paren_model }}{{ no_pos_encodings_underscore }}/babylm_{{ perturbation }}_{{ train_set }}_{{ paren_model }}{{ no_pos_encodings_underscore }}_seed{{ seed }}/artifacts
    run_dir: {{ ckpt_path }}/{{ perturbation }}_{{ train_set }}_{{ paren_model }}{{ no_pos_encodings_underscore }}/babylm_{{ perturbation }}_{{ train_set }}_{{ paren_model }}{{ no_pos_encodings_underscore }}_seed{{ seed }}/runs

# Save Effective Batch Size for Easy Handling ==> Main Code asserts infra + training_config results in this!
effective_bsz: 512

# Resume from Checkpoint
resume: false
resume_checkpoint: null

# List of frequencies at which to save checkpoints, provided as a list of two-element tuples:
#   - Frequency (`freq`) at which to save checkpoints (# steps)
#   - Bound (`until`) on global step for given frequency (checkpoint every `freq` steps until global step = `until`)
checkpoint_frequency:
    - [100, 3000]

# `torch.distributed` Default Infra Parameters -- to be overwritten by call to `torch.distributed.launch`
local_rank: -1
nnodes: -1
nproc_per_node: -1

# DeepSpeed Default Infra Parameters -- to be overwritten by call to `DeepSpeed`
num_gpus: -1
num_nodes: -1
world_size: -1

# Logging Parameters -- 10 = DEBUG, 20 = INFO, 30 = WARNING, 40 = ERROR, 50 = CRITICAL
log_level: 20

# Random Seed
seed: {{ seed }}
